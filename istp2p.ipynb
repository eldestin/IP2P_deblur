{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diffuser generation tour\n",
    "首先进行controlnet 源码详解和stable diffusion v1.5 的代码处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import DiffusionPipeline\n",
    "# pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.to(\"cuda\")\n",
    "# image = pipeline(\"A woman with 3D animated cartoon style\").images[0]\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct pix 2 pix 复现\n",
    "# 首先生成数据集\n",
    "import json\n",
    "import numpy as np\n",
    "human_written_prompt_path = \"/home/fhx/MyProject/multimodal/instructpix2pix/human-written-prompts.jsonl\"\n",
    "with open(human_written_prompt_path) as fp:\n",
    "    prompts = [json.loads(line) for line in fp]\n",
    "# get [index, data]\n",
    "prompts = np.array_split(list(enumerate(prompts)), 1)[0]\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, _ in unet.named_parameters():\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化stable diffusion pipeline -> decompose \n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel,EulerAncestralDiscreteScheduler\n",
    "from diffusers.schedulers.scheduling_euler_ancestral_discrete import EulerAncestralDiscreteSchedulerOutput\n",
    "# 初始化三个部分的sd\n",
    "def initialize_model(checkpoint_name):\n",
    "    vae = AutoencoderKL.from_pretrained(checkpoint_name, subfolder=\"vae\", use_safetensors=True)\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(checkpoint_name, subfolder = \"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(checkpoint_name, subfolder = \"text_encoder\", \n",
    "                                                use_safetensors=True)\n",
    "    unet = UNet2DConditionModel.from_pretrained(checkpoint_name, subfolder = \"unet\", \n",
    "                                                use_safetensors = True)\n",
    "    # scheduler = pipeline.scheduler\n",
    "    return vae, tokenizer, text_encoder, unet\n",
    "\n",
    "def image_process(image):\n",
    "    img_copy = image.copy()\n",
    "    for i in range(image.shape[0]):\n",
    "        image = img_copy[i, ...]\n",
    "        image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "        image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "        image = Image.fromarray(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EulerAncestralp2pscheduler(EulerAncestralDiscreteScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_train_timesteps: int = 1000,\n",
    "        beta_start: float = 0.0001,\n",
    "        beta_end: float = 0.02,\n",
    "        beta_schedule: str = \"linear\",\n",
    "        prediction_type: str = \"epsilon\"\n",
    "    ):\n",
    "        super().__init__(num_train_timesteps, beta_start, beta_end, beta_schedule, prediction_type = prediction_type)\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        model_output: torch.FloatTensor,\n",
    "        timestep,\n",
    "        sample: torch.FloatTensor,\n",
    "        generator = None,\n",
    "        return_dict: bool = True,\n",
    "    ):\n",
    "        if self.step_index is None:\n",
    "            self._init_step_index(timestep)\n",
    "\n",
    "        sigma = self.sigmas[self.step_index]\n",
    "\n",
    "        # Upcast to avoid precision issues when computing prev_sample\n",
    "        sample = sample.to(torch.float32)\n",
    "\n",
    "        # 1. compute predicted original sample (x_0) from sigma-scaled predicted noise\n",
    "        if self.config.prediction_type == \"epsilon\":\n",
    "            pred_original_sample = sample - sigma * model_output\n",
    "        elif self.config.prediction_type == \"v_prediction\":\n",
    "            # * c_out + input * c_skip\n",
    "            pred_original_sample = model_output * (-sigma / (sigma**2 + 1) ** 0.5) + (sample / (sigma**2 + 1))\n",
    "        elif self.config.prediction_type == \"sample\":\n",
    "            raise NotImplementedError(\"prediction_type not implemented yet: sample\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, or `v_prediction`\"\n",
    "            )\n",
    "\n",
    "        sigma_from = self.sigmas[self.step_index]\n",
    "        sigma_to = self.sigmas[self.step_index + 1]\n",
    "        sigma_up = (sigma_to**2 * (sigma_from**2 - sigma_to**2) / sigma_from**2) ** 0.5\n",
    "        sigma_down = (sigma_to**2 - sigma_up**2) ** 0.5\n",
    "\n",
    "        # 2. Convert to an ODE derivative\n",
    "        derivative = (sample - pred_original_sample) / sigma\n",
    "\n",
    "        dt = sigma_down - sigma\n",
    "\n",
    "        prev_sample = sample + derivative * dt\n",
    "\n",
    "        device = model_output.device\n",
    "        noise = torch.randn(model_output.shape, dtype=model_output.dtype, device=device, generator=generator)\n",
    "\n",
    "        prev_sample = prev_sample + noise * sigma_up\n",
    "\n",
    "        # Cast sample back to model compatible dtype\n",
    "        prev_sample = prev_sample.to(model_output.dtype)\n",
    "\n",
    "        # upon completion increase step index by one\n",
    "        self._step_index += 1\n",
    "\n",
    "        if not return_dict:\n",
    "            return (prev_sample,)\n",
    "\n",
    "        return EulerAncestralDiscreteSchedulerOutput(\n",
    "            prev_sample=prev_sample, pred_original_sample=pred_original_sample\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_super = EulerAncestralDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
    "scheduler_config = scheduler_super.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = EulerAncestralp2pscheduler(scheduler_config[\"num_train_timesteps\"],\n",
    "                                                  scheduler_config[\"beta_start\"],\n",
    "                                                  scheduler_config[\"beta_end\"],\n",
    "                                                  scheduler_config[\"beta_schedule\"],\n",
    "                                                  scheduler_config[\"prediction_type\"]\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对文本提示词进行编码\n",
    "# initialzie model\n",
    "\n",
    "# vae, tokenizer, text_encoder, unet = initialize_model(\"runwayml/stable-diffusion-v1-5\")\n",
    "# device = \"cuda\"\n",
    "# vae.to(device)\n",
    "# text_encoder.to(device)\n",
    "# unet.to(device)\n",
    "# # scheduler = EulerAncestralDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
    "# from tqdm import tqdm\n",
    "# for i, prompt in tqdm(prompts[:10]):\n",
    "#     height, width  = 512, 512\n",
    "#     num_inference_step = 25\n",
    "#     guidance_scale = 7.5\n",
    "#     generator = torch.Generator(device = device).manual_seed(0)\n",
    "#     batch_size = 1\n",
    "#     # get text input\n",
    "#     # print(prompt)\n",
    "#     text_input = tokenizer(\n",
    "#         [\"Generate similar scene in this batch: \" + prompt[\"input\"], prompt[\"output\"]], padding = \"max_length\", max_length = tokenizer.model_max_length, truncation = True,\n",
    "#         return_tensors = \"pt\"\n",
    "#     )\n",
    "#     # generate unconditional text embeddings for classifier-free guidance\n",
    "#     max_length = text_input.input_ids.shape[-1]\n",
    "#     uncond_input = tokenizer([\"\"] * 2 , padding = \"max_length\", max_length = max_length,\n",
    "#                              return_tensors = \"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         text_embd = text_encoder(text_input.input_ids.to(device))[0]\n",
    "#     # print(text_embd.shape)\n",
    "#         uncond_embeds = text_encoder(uncond_input.input_ids.to(device))[0]\n",
    "#         text_embeddings = torch.cat([uncond_embeds, text_embd])\n",
    "#     # create latent diffusion noise\n",
    "#     latents = torch.randn(\n",
    "#         (2, 4, height//8, width//8),\n",
    "#         generator = generator,\n",
    "#         device = device\n",
    "#     )\n",
    "#     # Denoising image\n",
    "#     # use default scheduler\n",
    "#     latents = latents * scheduler.init_noise_sigma\n",
    "#     scheduler.set_timesteps(num_inference_step)\n",
    "#     for t in tqdm(scheduler.timesteps):\n",
    "#         latent_model_input = torch.cat([latents] * 2)\n",
    "#         latent_model_input = scheduler.scale_model_input(latent_model_input, timestep = t)\n",
    "#         # predict noise residual \n",
    "#         with torch.no_grad():\n",
    "#             noise_pred = unet(latent_model_input, t, encoder_hidden_states = text_embeddings).sample\n",
    "#             noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "#             noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "#             latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "#     print(latents.shape)\n",
    "#     # decode image\n",
    "#     latents = 1 / 0.18215 * latents\n",
    "#     with torch.no_grad():\n",
    "#         image = vae.decode(latents).sample\n",
    "#     image_before_edit, image_after_edit = image_process(image[0,...]), image_process(image[1,...])\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset processing\n",
    "# import pandas as pd\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import os\n",
    "# path = \"/home/fhx/MyProject/multimodal/dataset/train-00000-of-00001-476d66d124561578.parquet\"\n",
    "# train_path = \"/home/fhx/MyProject/multimodal/dataset/ip2p/train\"\n",
    "# valid_path = \"/home/fhx/MyProject/multimodal/dataset/ip2p/valid\"\n",
    "# ip2p = pd.read_parquet(path)\n",
    "# # 取10% 作为validation\n",
    "# valid = ip2p.sample(int(ip2p.shape[0] * 0.1), axis = 0, random_state=0)\n",
    "# train = ip2p.iloc[~(ip2p.index.isin(valid.index))]\n",
    "# def preprocess(ip2p, path):\n",
    "#     before_path = os.path.join(path, \"before_edit\")\n",
    "#     after_path = os.path.join(path, \"after_edit\")\n",
    "#     prompt_path = os.path.join(path, \"prompt\")\n",
    "#     for i, (img1, img2) in enumerate(zip(ip2p[\"input_image\"], ip2p[\"edited_image\"])):\n",
    "#         before_edit_img = img1[\"bytes\"]\n",
    "#         after_edit_img = img2[\"bytes\"]\n",
    "#         # print(before_edit_img)\n",
    "#         # break\n",
    "#         before_edit_img_np = cv2.imdecode(np.frombuffer(before_edit_img, dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "#         # print(before_edit_img_np)\n",
    "#         # break\n",
    "#         after_edit_img_np = cv2.imdecode(np.frombuffer(after_edit_img, dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "#         cv2.imwrite(before_path + f\"/image_{i}.jpg\", before_edit_img_np)\n",
    "#         cv2.imwrite(after_path + f\"/image_{i}.jpg\", after_edit_img_np)\n",
    "#     # process text\n",
    "#     # prompt_path = \"/home/fhx/MyProject/multimodal/dataset/ip2p/prompt/\"\n",
    "#     for i, text in enumerate(ip2p[\"edit_prompt\"]):\n",
    "#         with open(os.path.join(prompt_path,f'text_{i}.txt'), \"w\") as f:\n",
    "#             f.write(text)\n",
    "#     print(\"Finish preprocessing.\")\n",
    "# preprocess(train, train_path)\n",
    "# preprocess(valid, valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instruct pix2pix pipeline \n",
    "# import torch\n",
    "# from diffusers import StableDiffusionInstructPix2PixPipeline\n",
    "# pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "#     \"timbrooks/instruct-pix2pix\", torch_dtype=torch.float16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n",
    "from transformers import CLIPModel, CLIPImageProcessor\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as tfs\n",
    "from torchvision.transforms import functional as FF\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "class IP2P_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, path, train, size = 320, format = \".jpg\", crop = True, tokenizer = None):\n",
    "        super(IP2P_dataset, self).__init__()\n",
    "        self.crop_size = size\n",
    "        self.format = format\n",
    "        self.train = train\n",
    "        self.before_edit_dir = os.listdir(os.path.join(path, \"before_edit\"))\n",
    "        # self.before_edit_imgs = [os.path.join()]\n",
    "        self.before_edit_imgs = os.path.join(path, \"before_edit\")\n",
    "        self.after_edit_dir = os.path.join(path, \"after_edit\")\n",
    "        self.prompt_dir = os.path.join(path, \"prompt\")\n",
    "        self.crop = crop\n",
    "        # print(self.before_edit_dir)\n",
    "        self.tokenizer = tokenizer\n",
    "    def __getitem__(self, idx):\n",
    "        before_edit = Image.open(os.path.join(self.before_edit_imgs, self.before_edit_dir[idx]))\n",
    "        name = self.before_edit_dir[idx].split(\"_\")[-1].split(\".jpg\")[0]\n",
    "        after_name, prompt_name = \"image_\" + name + self.format, \"text_\" + name + \".txt\"\n",
    "        after_edit = Image.open(os.path.join(self.after_edit_dir, after_name))\n",
    "        with open(os.path.join(self.prompt_dir, prompt_name)) as f:\n",
    "            prompt = f.read()\n",
    "        if self.crop:\n",
    "            i,j,h,w=tfs.RandomCrop.get_params(before_edit,output_size=(self.crop_size,self.crop_size))\n",
    "            before_edit=FF.crop(before_edit,i,j,h,w)\n",
    "            after_edit=FF.crop(after_edit,i,j,h,w)\n",
    "        before_edit, after_edit = self.aug_data(before_edit.convert(\"RGB\"), after_edit.convert(\"RGB\"))\n",
    "        before_edit = tfs.ToTensor()(before_edit)\n",
    "        after_edit = tfs.ToTensor()(after_edit)\n",
    "        prompt = self.tokenizer(\n",
    "            prompt, padding = \"max_length\", max_length = self.tokenizer.model_max_length, truncation = True,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "        un_cond = self.tokenizer([\"\"] , padding = \"max_length\", max_length = self.tokenizer.model_max_length,\n",
    "                             return_tensors = \"pt\")\n",
    "        return before_edit, after_edit, prompt, un_cond\n",
    "\n",
    "    def aug_data(self, data, target):\n",
    "        if self.train:\n",
    "            rand_hor=random.randint(0,1)\n",
    "            rand_rot=random.randint(0,3)\n",
    "            data=tfs.RandomHorizontalFlip(rand_hor)(data)\n",
    "            target=tfs.RandomHorizontalFlip(rand_hor)(target)\n",
    "            if rand_rot:\n",
    "                data = FF.rotate(data, 90 * rand_rot)\n",
    "                target = FF.rotate(target, 90 * rand_rot)\n",
    "        return data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.before_edit_dir)\n",
    "    \n",
    "\n",
    "def image_process(img):\n",
    "    img_ls = []\n",
    "    for i in range(img.shape[0]):\n",
    "        image = img[i, ...]\n",
    "        image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "        image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "        image = Image.fromarray(image)\n",
    "        img_ls.append(image)\n",
    "    return img_ls\n",
    "# with torch.no_grad():\n",
    "#     vae, unet, text_encoder, scheduler = pipe.vae, pipe.unet, pipe.text_encoder, pipe.scheduler\n",
    "#     ds = IP2P_dataset(path, True, tokenizer=tokenizer, size = 512)\n",
    "#     loader = DataLoader(ds, batch_size=4)\n",
    "#     device = \"cuda\"\n",
    "#     guidance_scale = 7.5\n",
    "#     image_guidance_scale = 1.5\n",
    "#     vae.to(device).to(torch.float16)\n",
    "#     unet.to(device).to(torch.float16)\n",
    "#     text_encoder.to(device)\n",
    "#     batch = next(iter(loader))\n",
    "#     x, gt, prompt, un_cond = batch\n",
    "#     x, gt = x.to(torch.float16).to(device), gt.to(torch.float16).to(device)\n",
    "#     bs, c, h, w = x.shape\n",
    "#     generator = torch.Generator(device = device).manual_seed(0)\n",
    "#     # text guide\n",
    "#     text_embedding_hidden = text_encoder(prompt[\"input_ids\"].to(device))[0]\n",
    "#     # uncond text guide\n",
    "#     null_conditioning = text_encoder(un_cond[\"input_ids\"].to(device))[0]  \n",
    "#     # final text embd\n",
    "#     prompt_embd = torch.cat([text_embedding_hidden, null_conditioning, null_conditioning])\n",
    "\n",
    "#     # image final guide\n",
    "#     # source guide\n",
    "#     source_imge_embd = vae.encode(x).latent_dist.mode()\n",
    "#     vae_h, vae_w = source_imge_embd.shape[-2], source_imge_embd.shape[-1]\n",
    "#     un_cond_image =  torch.zeros_like(source_imge_embd)\n",
    "#     final_image_embd = torch.cat([source_imge_embd, un_cond_image, un_cond_image])\n",
    "\n",
    "#     # got latent representation\n",
    "#     latents = torch.randn(\n",
    "#     (bs, 4, vae_h, vae_w),\n",
    "#         generator = generator,\n",
    "#         device = device\n",
    "#     ).to(torch.float16)\n",
    "#     latents = latents * scheduler.init_noise_sigma\n",
    "#     scheduler.set_timesteps(4)\n",
    "#     for t in tqdm(scheduler.timesteps):\n",
    "#         # avoid doing three forward step\n",
    "#         # do classifier free guidance\n",
    "#         latent_model_input = torch.cat([latents] * 3)\n",
    "#         latent_model_input = scheduler.scale_model_input(latent_model_input, timestep = t)\n",
    "#         # concat latent and images latents\n",
    "#         latent_model_input = torch.cat([latent_model_input, final_image_embd], dim = 1)\n",
    "\n",
    "#         noise_pred = unet(latent_model_input, t, encoder_hidden_states = prompt_embd).sample\n",
    "#         noise_pred_text, noise_pred_image, noise_pred_uncond = noise_pred.chunk(3)\n",
    "#         noise_pred = (\n",
    "#             noise_pred_uncond \n",
    "#             + guidance_scale * (noise_pred_text - noise_pred_image)\n",
    "#             + image_guidance_scale * (noise_pred_image - noise_pred_uncond)\n",
    "#         )\n",
    "#         latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "#     print(latents.shape)\n",
    "#     # decode image\n",
    "#     latents = 1 / 0.18215 * latents\n",
    "#     edited_image = vae.decode(latents).sample\n",
    "#     edited_image = image_process(edited_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning training code\n",
    "from typing import Any\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint,LearningRateMonitor\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import lpips\n",
    "from utils.EMA import EMA as EMA_callback\n",
    "from utils.optimizer import get_cosine_schedule_with_warmup\n",
    "from utils.imgqual_utils import PSNR, SSIM\n",
    "from utils.loss import L1_Charbonnier_loss\n",
    "from transformers import CLIPModel, CLIPImageProcessor\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "class Instruct_pix2pix(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(Instruct_pix2pix, self).__init__()\n",
    "        self.params = params\n",
    "        self.max_steps = self.params.Trainer.max_steps\n",
    "        self.save_path = \"/home/fhx/MyProject/multimodal/instructpix2pix/diffusers/training_ip2p/save_img\"\n",
    "        # self.initlr = self.params.Trainer.initlr\n",
    "        self.train_datasets = self.params.Trainer.train_datasets\n",
    "        self.train_batchsize = self.params.Trainer.train_bs\n",
    "        self.validation_datasets = self.params.Val.val_datasets\n",
    "        self.val_batchsize = self.params.Val.val_bs\n",
    "        self.val_crop = True\n",
    "\n",
    "        self.conditioning_drop_rate = self.params.Model.conditioning_drop_rate\n",
    "        self.initlr = self.params.Trainer.initlr #initial learning\n",
    "        self.crop_size = self.params.Trainer.crop_size #random crop size\n",
    "        self.num_workers = self.params.Trainer.num_workers\n",
    "\n",
    "        \n",
    "\n",
    "        self.loss = L1_Charbonnier_loss()\n",
    "        self.noise_scheduler = DDIMScheduler(**self.params.scheduler)\n",
    "        self.noise_scheduler.set_timesteps(self.params.Model.num_test_timesteps)\n",
    "        self.vae, self.tokenizer, self.text_encoder, self.unet = self.initialize_model(self.params.Model.checkpoint)\n",
    "        print('training num:',self.train_dataloader().__len__())\n",
    "        print('validation num:',self.val_dataloader().__len__())\n",
    "        self.automatic_optimization = False\n",
    "        self.lpips_fn = lpips.LPIPS(net='alex')\n",
    "        self.mae = nn.L1Loss()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # validation model\n",
    "        # compute clip similarity\n",
    "        model_ID = \"openai/clip-vit-base-patch16\"\n",
    "        self.clipmodel = CLIPModel.from_pretrained(model_ID)\n",
    "        self.clipprocessor = CLIPImageProcessor.from_pretrained(model_ID)\n",
    "        self.guidance_scale = self.params.Model.guidance_scale\n",
    "        self.image_guidance_scale = self.params.Model.image_guidance_scale\n",
    "        # data, gt, _, _ = next(iter(loader))\n",
    "        # data, gt = processor(images=data, return_tensors = \"pt\"), processor(images=gt, return_tensors = \"pt\")\n",
    "        # embedding_a, embedding_b = model.get_image_features(data[\"pixel_values\"]), model.get_image_features(gt[\"pixel_values\"])\n",
    "        # clip metric\n",
    "        self.model_ID = \"openai/clip-vit-base-patch16\"\n",
    "        self.clip_model, self.clip_processor = self.initialize_clip_metrics(self.model_ID)\n",
    "        # FID metric\n",
    "        self.fid = FrechetInceptionDistance(feature = 192).to(self.device)\n",
    "    def image_process(self, img):\n",
    "        img_ls = []\n",
    "        for i in range(img.shape[0]):\n",
    "            image = img[i, ...]\n",
    "            image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "            image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "            image = Image.fromarray(image)\n",
    "            img_ls.append(image)\n",
    "        return img_ls\n",
    "    \n",
    "    def initialize_clip_metrics(self, checkpoint_name):\n",
    "        model = CLIPModel.from_pretrained(checkpoint_name)\n",
    "        processor = CLIPImageProcessor.from_pretrained(checkpoint_name)\n",
    "        # data, gt = processor(images=edited_image, return_tensors = \"pt\"), processor(images=gt, return_tensors = \"pt\")\n",
    "        # embedding_a, embedding_b = model.get_image_features(data[\"pixel_values\"]), model.get_image_features(gt[\"pixel_values\"])\n",
    "        model.requires_grad_(False)\n",
    "        return model, processor\n",
    "\n",
    "    def compute_clip_metrics(self, x, gt):\n",
    "        x, gt = self.clip_processor(images=x, return_tensors = \"pt\"), self.clip_processor(images=gt, return_tensors = \"pt\")\n",
    "        embedding_x, embedding_gt = self.clip_model.get_image_features(x[\"pixel_values\"]), self.clip_model.get_image_features(gt[\"pixel_values\"])\n",
    "        cos_sim = torch.nn.functional.cosine_similarity(embedding_x, embedding_gt)\n",
    "        return torch.mean(cos_sim).item()\n",
    "    \n",
    "    def initialize_model(self, checkpoint_name):\n",
    "        vae = AutoencoderKL.from_pretrained(checkpoint_name, subfolder=\"vae\", use_safetensors=True)\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(checkpoint_name, subfolder = \"tokenizer\")\n",
    "        text_encoder = CLIPTextModel.from_pretrained(checkpoint_name, subfolder = \"text_encoder\", \n",
    "                                                    use_safetensors=True)\n",
    "        unet = UNet2DConditionModel.from_pretrained(checkpoint_name, subfolder = \"unet\", \n",
    "                                                    use_safetensors = True)\n",
    "        \n",
    "        # modified unet\n",
    "        in_channels, out_channel = 8, unet.conv_in.out_channels\n",
    "        unet.register_to_config(in_channels = in_channels)\n",
    "        with torch.no_grad():\n",
    "            new_conv_in = nn.Conv2d(\n",
    "                in_channels=in_channels, out_channels=out_channel, kernel_size=unet.conv_in.kernel_size,\n",
    "                stride=unet.conv_in.stride, padding=unet.conv_in.padding\n",
    "            )\n",
    "            # initialize to zero\n",
    "            new_conv_in.weight.zero_()\n",
    "            # copy weight for the first 4 channels\n",
    "            new_conv_in.weight[:, :4, :, :].copy_(unet.conv_in.weight)\n",
    "            unet.conv_in = new_conv_in\n",
    "        vae.requires_grad_(False)\n",
    "        text_encoder.requires_grad_(False)\n",
    "        return vae, tokenizer, text_encoder, unet\n",
    "    \n",
    "    def lpips_score_fn(self,x,gt):\n",
    "        self.lpips_fn.to(self.device)\n",
    "        lp_score = self.lpips_fn(\n",
    "            gt * 2 - 1, x * 2 - 1\n",
    "        )\n",
    "        return torch.mean(lp_score).item()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        parameters = [\n",
    "            {\"params\": self.unet.parameters()}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(parameters, lr=self.initlr,betas=[0.9,0.999], eps = 1e-6, weight_decay=0.01)\n",
    "        scheduler2 = get_cosine_schedule_with_warmup(optimizer, 200,self.max_steps) #self.max_steps*0.02,\n",
    "        \n",
    "        return [optimizer], [scheduler2]\n",
    "    \n",
    "    def add_fid(self, pred, gt):\n",
    "        self.fid.update(pred, real = False)\n",
    "        self.fid.update(gt, real = True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        opt = self.optimzers()\n",
    "        opt.zero_grad()\n",
    "        x, gt, prompt, un_cond = batch\n",
    "        bs, c, h, w = x.shape\n",
    "        # got latent representation\n",
    "        latents = self.vae.encode(gt).latent_dist.sample()\n",
    "        latents = latents * self.vae.config.scaling_factor\n",
    "        # generate noise\n",
    "        noise = torch.randn_like(latents).to(self.device)\n",
    "        timesteps = torch.randint(0, self.noise_scheduler.config.num_train_timesteps, (bs,), device=self.device).long()\n",
    "        noisy_img = self.noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        \n",
    "        # get text embedding \n",
    "        text_embedding_hidden = self.text_encoder(prompt[\"input_ids\"])[0]\n",
    "        # 使用众数 mode as image input guidance\n",
    "        source_imge_embd = self.vae.encode(x).latent_dist.mode()\n",
    "\n",
    "        # classifier free guidance introduced in the source paper\n",
    "        generator = torch.Generator(device = self.device).manual_seed(10)\n",
    "\n",
    "        random_p = torch.rand(bs, device = self.device, generator = generator)\n",
    "        # configure prompt mask\n",
    "        prompt_mask = random_p < 2 * self.conditioning_drop_rate\n",
    "        prompt_mask = prompt_mask.reshape(bs, 1, 1)\n",
    "        \n",
    "        # text conditioning  \n",
    "        null_conditioning = self.text_encoder(un_cond)[0]\n",
    "        # if mask null condtion embd else text embed\n",
    "        text_embedding_hidden = torch.where(prompt_mask, null_conditioning, text_embedding_hidden)\n",
    "\n",
    "        # Sample mask for original images\n",
    "        image_mask_dtype = source_imge_embd.dtype\n",
    "        image_mask = 1 - (\n",
    "            (random_p >= self.conditioning_drop_rate).to(image_mask_dtype)\n",
    "             * (random_p < 3 * self.conditioning_drop_rate).to*(image_mask_dtype)\n",
    "        )\n",
    "        image_mask = image_mask.reshape(bs, 1, 1, 1)\n",
    "        # final image conditioning\n",
    "        source_imge_embd = image_mask * source_imge_embd\n",
    "\n",
    "        # final guidance input:\n",
    "        noisy_img = torch.cat([noisy_img, source_imge_embd], dim = 1)\n",
    "        model_pred = self.unet(noisy_img, timesteps, text_embedding_hidden).sample\n",
    "        target = noise if self.noise_scheduler.config.prediction_type == \"epsilon\" else latents\n",
    "        loss = self.loss(model_pred, target)\n",
    "        self.manual_backward(loss)\n",
    "        opt.step()\n",
    "        sch = self.lr_schedulers()\n",
    "        sch.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.log(\"train_loss\", loss, prog_bar = True)\n",
    "        \n",
    "        return {\"loss\", loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, gt, prompt, un_cond = batch\n",
    "        bs, c, h, w = x.shape\n",
    "       \n",
    "        # text guide\n",
    "        text_embedding_hidden = self.text_encoder(prompt[\"input_ids\"])[0]\n",
    "        # uncond text guide\n",
    "        null_conditioning = self.text_encoder(un_cond[\"input_ids\"])[0]\n",
    "        # final text embd\n",
    "        prompt_embd = torch.cat([text_embedding_hidden, null_conditioning, null_conditioning])\n",
    "        \n",
    "        # image final guide\n",
    "        # source guide\n",
    "        source_imge_embd = self.vae.encode(x).latent_dist.mode()\n",
    "        un_cond_image =  torch.zeros_like(source_imge_embd)\n",
    "        final_image_embd = torch.cat([source_imge_embd, un_cond_image, un_cond_image])\n",
    "        generator = torch.Generator(device = self.device).manual_seed(10)\n",
    "\n",
    "        # got latent representation\n",
    "        latents = torch.randn(\n",
    "        (bs, 4, source_imge_embd.shape[-2], source_imge_embd.shape[-1]),\n",
    "            generator = generator,\n",
    "            device = self.device\n",
    "        )\n",
    "        latents = latents * self.noise_scheduler.init_noise_sigma\n",
    "        for t in tqdm(self.noise_scheduler.timesteps):\n",
    "            # avoid doing three forward step\n",
    "            # do classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 3)\n",
    "            latent_model_input = self.noise_scheduler.scale_model_input(latent_model_input, timestep = t)\n",
    "            # concat latent and images latents\n",
    "            latent_model_input = torch.cat([latent_model_input, final_image_embd], dim = 1)\n",
    "\n",
    "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states = prompt_embd).sample\n",
    "            noise_pred_text, noise_pred_image, noise_pred_uncond = noise_pred.chunk(3)\n",
    "            noise_pred = (\n",
    "                noise_pred_uncond \n",
    "                + self.guidance_scale * (noise_pred_text - noise_pred_image)\n",
    "                + self.image_guidance_scale * (noise_pred_image - noise_pred_uncond)\n",
    "            )\n",
    "            latents = self.noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        print(latents.shape)\n",
    "        # decode image\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        edited_image = self.vae.decode(latents).sample\n",
    "        edited_image_ls = self.image_process(edited_image)\n",
    "        cos_sim = self.compute_clip_metrics(edited_image_ls, gt)\n",
    "        lpips_score = self.lpips_score_fn(edited_image.float(), gt.float())\n",
    "        self.add_fid((edited_image*255).to(torch.uint8), (gt*255).to(torch.uint8))\n",
    "        # log metric\n",
    "        mae = self.mae(edited_image, gt)\n",
    "        self.log(\"MAE\", mae, sync_dist = True)\n",
    "        self.log(\"LPIPS_score\", lpips_score, sync_dist = True)\n",
    "        self.log(\"Clip image sim\", cos_sim, sync_dist = True)\n",
    "        print(\"Finish first step validation. \")\n",
    "        return {\"LPIPS_score\":lpips_score, \"Clip_image_sim\":cos_sim, \"MAE\":mae}\n",
    "        \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        # calculate fid\n",
    "        fid_score = self.fid.compute()\n",
    "        # clear cache\n",
    "        self.fid.reset()\n",
    "        self.log(\"Epoch FID: \", fid_score, sync_dist = True)\n",
    "        return \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        train_set = IP2P_dataset(self.train_datasets,train=True,size=self.crop_size, tokenizer=self.tokenizer)\n",
    "        train_loader = DataLoader(train_set, batch_size=self.train_batchsize, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_set = IP2P_dataset(self.validation_datasets,train=False,size=200,crop=True, tokenizer=self.tokenizer)\n",
    "        val_loader = DataLoader(val_set, batch_size=self.val_batchsize, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "        return val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "import yaml\n",
    "\n",
    "ddp = DDPStrategy(process_group_backend=\"nccl\", find_unused_parameters = True)\n",
    "config_path = r'/home/fhx/MyProject/multimodal/instructpix2pix/diffusers/training_ip2p/option/IP2P_edited.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "config = EasyDict(params)\n",
    "model = Instruct_pix2pix(config)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Clip_image_sim',\n",
    "    filename='epoch{epoch:02d}-Clip_image_sim-{Clip_image_sim:.3f}-lpips_score-{lpips_score:.4f}',\n",
    "    auto_insert_metric_name=False,   \n",
    "    every_n_epochs=1,\n",
    "    save_top_k=3,\n",
    "    mode = \"max\",\n",
    "    save_last=True\n",
    "    )\n",
    "ema_ck = EMA_callback(decay = 0.999)\n",
    "output_dir = '/home/fhx/MyProject/multimodal/instructpix2pix/diffusers/training_ip2p/lgs/NAFNetguidancecat'\n",
    "logger = TensorBoardLogger(name=config.log_name,save_dir = output_dir )\n",
    "lr_monitor_callback = LearningRateMonitor(logging_interval='step')\n",
    "trainer = pl.Trainer(\n",
    "    check_val_every_n_epoch=config.Trainer.check_val_every_n_epoch,\n",
    "    max_steps=config.Trainer.max_steps,\n",
    "    accelerator=config.Trainer.accelerator,\n",
    "    devices=config.Trainer.devices,\n",
    "    precision=config.Trainer.precision,\n",
    "    accumulate_grad_batches = config.Trainer.accumulate_grad_batches,\n",
    "    logger=logger,\n",
    "    strategy=\"ddp_notebook\",\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=config.Trainer.log_every_n_steps,\n",
    "    callbacks = [checkpoint_callback,lr_monitor_callback, ema_ck]\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
